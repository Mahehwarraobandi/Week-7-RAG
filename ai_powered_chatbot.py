# -*- coding: utf-8 -*-
"""Ai-Powered ChatBot

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Hdv_nvgb_T8kZB4CUj5Qv3wwZq0CxWLV
"""

pip install streamlit sentence-transformers faiss-cpu numpy pandas

pip install faiss-cpu

# Commented out IPython magic to ensure Python compatibility.
# %%writefile retriever.py
# import os
# import pandas as pd
# import numpy as np
# import faiss
# from sentence_transformers import SentenceTransformer
# 
# # Load dataset (Ensure "job_postings.csv" is uploaded in Colab)
# file_path = "/content/job_postings.csv"
# df = pd.read_csv(file_path)
# 
# # Load the embedding model
# model = SentenceTransformer("all-MiniLM-L6-v2")
# 
# # Function to generate embeddings
# def get_embedding(text):
#     return model.encode(text).tolist()
# 
# # Prepare job descriptions for embedding
# df["job_description"] = df.apply(lambda row: f"Job Title: {row['job_title']}, "
#                                              f"Company: {row['company']}, "
#                                              f"Location: {row['job_location']}, "
#                                              f"Type: {row['job_type']}, "
#                                              f"Link: {row['job_link']}", axis=1)
# 
# # Generate embeddings
# df["embedding"] = df["job_description"].apply(get_embedding)
# 
# # Convert embeddings to NumPy array
# embeddings = np.array(df["embedding"].tolist())
# 
# # Ensure directory exists
# os.makedirs("/content/data", exist_ok=True)
# 
# # Initialize FAISS index
# dimension = len(embeddings[0])
# faiss_index = faiss.IndexFlatL2(dimension)
# faiss_index.add(embeddings)
# 
# # Save FAISS index & job data
# faiss.write_index(faiss_index, "/content/data/faiss_index.bin")
# df.to_pickle("/content/data/job_postings.pkl")
# 
# print("‚úÖ FAISS index and embeddings stored successfully in /content/data/")
# 
# # Function to retrieve jobs from FAISS index
# def retrieve_jobs(query, k=3):
#     query_embedding = np.array(model.encode(query)).reshape(1, -1)
#     _, indices = faiss_index.search(query_embedding, k)
# 
#     results = []
#     for idx in indices[0]:
#         job_info = df.iloc[idx].to_dict()
#         results.append(job_info)
# 
#     return results
#

# Commented out IPython magic to ensure Python compatibility.
# %run retriever.py

query = "Machine Learning Engineer jobs"
results = retrieve_jobs(query)

for job in results:
    print("Job Title:", job["job_title"])
    print("Company:", job["company"])
    print("Location:", job["job_location"])
    print("Link:", job["job_link"])
    print("-" * 50)

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# 
# # Use retrieve_jobs function from the same notebook
# from retriever import retrieve_jobs
# 
# # Chatbot function
# def generate_response(user_query):
#     retrieved_jobs = retrieve_jobs(user_query, k=3)
# 
#     if not retrieved_jobs:
#         return "Sorry, I couldn't find any relevant job postings."
# 
#     job_texts = "\n\n".join([f"üìå Job Title: {job['job_title']}\nüè¢ Company: {job['company']}\nüìç Location: {job['job_location']}\nüîó [Apply Here]({job['job_link']})"
#                               for job in retrieved_jobs])
# 
#     response = f"""
#     Here are the top matching job postings:
# 
#     {job_texts}
# 
#     Check the job links for more details.
#     """
# 
#     return response
# 
# # Streamlit UI
# st.set_page_config(page_title="üîç Job Search Chatbot", layout="centered")
# 
# st.title("üîç AI-Powered Job Search Chatbot")
# st.write("Ask me about job postings!")
# 
# user_input = st.text_input("Enter your query:")
# 
# if st.button("Send"):
#     if user_input:
#         response = generate_response(user_input)
#         st.write("ü§ñ Chatbot:", response)
#

!wget https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O cloudflared
!chmod +x cloudflared

!streamlit run app.py & ./cloudflared tunnel --url http://localhost:8501

